{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d0c6b1b",
   "metadata": {},
   "source": [
    "## PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0359d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create a session:\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Permission').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a2ad9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-BFEIRED:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Permission</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x27a1c2d50a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba47222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries:\n",
    "import pyspark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a88e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 193 entries, 0 to 192\n",
      "Data columns (total 6 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   country                       193 non-null    object \n",
      " 1   beer_servings                 193 non-null    int64  \n",
      " 2   spirit_servings               193 non-null    int64  \n",
      " 3   wine_servings                 193 non-null    int64  \n",
      " 4   total_litres_of_pure_alcohol  193 non-null    float64\n",
      " 5   continent                     170 non-null    object \n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 9.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#Read dataframe in pandas:\n",
    "df = pd.read_csv(\"Alcohol.csv\")\n",
    "\n",
    "\n",
    "#To check the datatypes and non null count:\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2e1758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0='country', _c1='beer_servings', _c2='spirit_servings', _c3='wine_servings', _c4='total_litres_of_pure_alcohol', _c5='continent')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To read the csv file using spark:\n",
    "dfspark =spark.read.csv(\"Alcohol.csv\")\n",
    "dfspark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "732e5f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To read the csv using spark and assigning header names also:\n",
    "dfspark = spark.read.option('header','true').csv(\"Alcohol.csv\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e998afe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(country='Afghanistan', beer_servings=0, spirit_servings=0, wine_servings=0, total_litres_of_pure_alcohol=0.0, continent='AS'),\n",
       " Row(country='Albania', beer_servings=89, spirit_servings=132, wine_servings=54, total_litres_of_pure_alcohol=4.9, continent='EU'),\n",
       " Row(country='Algeria', beer_servings=25, spirit_servings=0, wine_servings=14, total_litres_of_pure_alcohol=0.7, continent='AF'),\n",
       " Row(country='Andorra', beer_servings=245, spirit_servings=138, wine_servings=312, total_litres_of_pure_alcohol=12.4, continent='EU'),\n",
       " Row(country='Angola', beer_servings=217, spirit_servings=57, wine_servings=45, total_litres_of_pure_alcohol=5.9, continent='AF')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To read the csv\n",
    "dfspark =spark.read.csv(\"Alcohol.csv\", header=True, inferSchema=True)\n",
    "dfspark.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cc20f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To check the datatype of dataframe\n",
    "type(dfspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db50af04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- beer_servings: integer (nullable = true)\n",
      " |-- spirit_servings: integer (nullable = true)\n",
      " |-- wine_servings: integer (nullable = true)\n",
      " |-- total_litres_of_pure_alcohol: double (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the schema\n",
    "dfspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dc7bb5",
   "metadata": {},
   "source": [
    "# PySpark basic operations:\n",
    "    1. To select all the columns \n",
    "    2. To select multiple columns\n",
    "    3. To check the data types of columns\n",
    "    4. To add the column and drop the column\n",
    "    5. Describe functionality\n",
    "    6. Column rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78c303cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['country',\n",
       " 'beer_servings',\n",
       " 'spirit_servings',\n",
       " 'wine_servings',\n",
       " 'total_litres_of_pure_alcohol',\n",
       " 'continent']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4bc6036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+\n",
      "|          country|beer_servings|spirit_servings|wine_servings|total_litres_of_pure_alcohol|continent|\n",
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+\n",
      "|      Afghanistan|            0|              0|            0|                         0.0|       AS|\n",
      "|          Albania|           89|            132|           54|                         4.9|       EU|\n",
      "|          Algeria|           25|              0|           14|                         0.7|       AF|\n",
      "|          Andorra|          245|            138|          312|                        12.4|       EU|\n",
      "|           Angola|          217|             57|           45|                         5.9|       AF|\n",
      "|Antigua & Barbuda|          102|            128|           45|                         4.9|       NA|\n",
      "|        Argentina|          193|             25|          221|                         8.3|       SA|\n",
      "|          Armenia|           21|            179|           11|                         3.8|       EU|\n",
      "|        Australia|          261|             72|          212|                        10.4|       OC|\n",
      "|          Austria|          279|             75|          191|                         9.7|       EU|\n",
      "|       Azerbaijan|           21|             46|            5|                         1.3|       EU|\n",
      "|          Bahamas|          122|            176|           51|                         6.3|       NA|\n",
      "|          Bahrain|           42|             63|            7|                         2.0|       AS|\n",
      "|       Bangladesh|            0|              0|            0|                         0.0|       AS|\n",
      "|         Barbados|          143|            173|           36|                         6.3|       NA|\n",
      "|          Belarus|          142|            373|           42|                        14.4|       EU|\n",
      "|          Belgium|          295|             84|          212|                        10.5|       EU|\n",
      "|           Belize|          263|            114|            8|                         6.8|       NA|\n",
      "|            Benin|           34|              4|           13|                         1.1|       AF|\n",
      "|           Bhutan|           23|              0|            0|                         0.4|       AS|\n",
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To check the dataframe\n",
    "dfspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce66aef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|          country|\n",
      "+-----------------+\n",
      "|      Afghanistan|\n",
      "|          Albania|\n",
      "|          Algeria|\n",
      "|          Andorra|\n",
      "|           Angola|\n",
      "|Antigua & Barbuda|\n",
      "|        Argentina|\n",
      "|          Armenia|\n",
      "|        Australia|\n",
      "|          Austria|\n",
      "|       Azerbaijan|\n",
      "|          Bahamas|\n",
      "|          Bahrain|\n",
      "|       Bangladesh|\n",
      "|         Barbados|\n",
      "|          Belarus|\n",
      "|          Belgium|\n",
      "|           Belize|\n",
      "|            Benin|\n",
      "|           Bhutan|\n",
      "+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To select the particular column \n",
    "dfspark.select('country').show() #By default it shows first 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5726e9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+\n",
      "|          country|beer_servings|\n",
      "+-----------------+-------------+\n",
      "|      Afghanistan|            0|\n",
      "|          Albania|           89|\n",
      "|          Algeria|           25|\n",
      "|          Andorra|          245|\n",
      "|           Angola|          217|\n",
      "|Antigua & Barbuda|          102|\n",
      "|        Argentina|          193|\n",
      "|          Armenia|           21|\n",
      "|        Australia|          261|\n",
      "|          Austria|          279|\n",
      "+-----------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To select the multiple columns with first 10 rows\n",
    "dfspark.select(['country','beer_servings']).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "801645c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+---------------+\n",
      "|          country|beer_servings|spirit_servings|\n",
      "+-----------------+-------------+---------------+\n",
      "|      Afghanistan|            0|              0|\n",
      "|          Albania|           89|            132|\n",
      "|          Algeria|           25|              0|\n",
      "|          Andorra|          245|            138|\n",
      "|           Angola|          217|             57|\n",
      "|Antigua & Barbuda|          102|            128|\n",
      "|        Argentina|          193|             25|\n",
      "|          Armenia|           21|            179|\n",
      "|        Australia|          261|             72|\n",
      "|          Austria|          279|             75|\n",
      "+-----------------+-------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To select the multiple columns with first 10 rows\n",
    "dfspark.select(['country','beer_servings','spirit_servings']).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c792be0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('country', 'string'),\n",
       " ('beer_servings', 'int'),\n",
       " ('spirit_servings', 'int'),\n",
       " ('wine_servings', 'int'),\n",
       " ('total_litres_of_pure_alcohol', 'double'),\n",
       " ('continent', 'string')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To check dtypes of all the columns:\n",
    "dfspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85b45f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------+-----------------+-----------------+----------------------------+---------+\n",
      "|summary|    country|     beer_servings|  spirit_servings|    wine_servings|total_litres_of_pure_alcohol|continent|\n",
      "+-------+-----------+------------------+-----------------+-----------------+----------------------------+---------+\n",
      "|  count|        193|               193|              193|              193|                         193|      193|\n",
      "|   mean|       null|106.16062176165804|80.99481865284974|49.45077720207254|           4.717098445595855|     null|\n",
      "| stddev|       null| 101.1431025393134|88.28431210968618|79.69759845763012|           3.773298164356082|     null|\n",
      "|    min|Afghanistan|                 0|                0|                0|                         0.0|       AF|\n",
      "|    max|   Zimbabwe|               376|              438|              370|                        14.4|       SA|\n",
      "+-------+-----------+------------------+-----------------+-----------------+----------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Describe function\n",
    "dfspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08017089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+\n",
      "|          country|beer_servings|spirit_servings|wine_servings|total_litres_of_pure_alcohol|continent|\n",
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+\n",
      "|      Afghanistan|            0|              0|            0|                         0.0|       AS|\n",
      "|          Albania|           89|            132|           54|                         4.9|       EU|\n",
      "|          Algeria|           25|              0|           14|                         0.7|       AF|\n",
      "|          Andorra|          245|            138|          312|                        12.4|       EU|\n",
      "|           Angola|          217|             57|           45|                         5.9|       AF|\n",
      "|Antigua & Barbuda|          102|            128|           45|                         4.9|       NA|\n",
      "|        Argentina|          193|             25|          221|                         8.3|       SA|\n",
      "|          Armenia|           21|            179|           11|                         3.8|       EU|\n",
      "|        Australia|          261|             72|          212|                        10.4|       OC|\n",
      "|          Austria|          279|             75|          191|                         9.7|       EU|\n",
      "|       Azerbaijan|           21|             46|            5|                         1.3|       EU|\n",
      "|          Bahamas|          122|            176|           51|                         6.3|       NA|\n",
      "|          Bahrain|           42|             63|            7|                         2.0|       AS|\n",
      "|       Bangladesh|            0|              0|            0|                         0.0|       AS|\n",
      "|         Barbados|          143|            173|           36|                         6.3|       NA|\n",
      "|          Belarus|          142|            373|           42|                        14.4|       EU|\n",
      "|          Belgium|          295|             84|          212|                        10.5|       EU|\n",
      "|           Belize|          263|            114|            8|                         6.8|       NA|\n",
      "|            Benin|           34|              4|           13|                         1.1|       AF|\n",
      "|           Bhutan|           23|              0|            0|                         0.4|       AS|\n",
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2556312f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+-------------------+\n",
      "|          country|beer_servings|spirit_servings|wine_servings|total_litres_of_pure_alcohol|continent|wine_servings_after|\n",
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+-------------------+\n",
      "|      Afghanistan|            0|              0|            0|                         0.0|       AS|                 10|\n",
      "|          Albania|           89|            132|           54|                         4.9|       EU|                 64|\n",
      "|          Algeria|           25|              0|           14|                         0.7|       AF|                 24|\n",
      "|          Andorra|          245|            138|          312|                        12.4|       EU|                322|\n",
      "|           Angola|          217|             57|           45|                         5.9|       AF|                 55|\n",
      "|Antigua & Barbuda|          102|            128|           45|                         4.9|       NA|                 55|\n",
      "|        Argentina|          193|             25|          221|                         8.3|       SA|                231|\n",
      "|          Armenia|           21|            179|           11|                         3.8|       EU|                 21|\n",
      "|        Australia|          261|             72|          212|                        10.4|       OC|                222|\n",
      "|          Austria|          279|             75|          191|                         9.7|       EU|                201|\n",
      "|       Azerbaijan|           21|             46|            5|                         1.3|       EU|                 15|\n",
      "|          Bahamas|          122|            176|           51|                         6.3|       NA|                 61|\n",
      "|          Bahrain|           42|             63|            7|                         2.0|       AS|                 17|\n",
      "|       Bangladesh|            0|              0|            0|                         0.0|       AS|                 10|\n",
      "|         Barbados|          143|            173|           36|                         6.3|       NA|                 46|\n",
      "|          Belarus|          142|            373|           42|                        14.4|       EU|                 52|\n",
      "|          Belgium|          295|             84|          212|                        10.5|       EU|                222|\n",
      "|           Belize|          263|            114|            8|                         6.8|       NA|                 18|\n",
      "|            Benin|           34|              4|           13|                         1.1|       AF|                 23|\n",
      "|           Bhutan|           23|              0|            0|                         0.4|       AS|                 10|\n",
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Adding column in dataframe:\n",
    "dfspark.withColumn('wine_servings_after',dfspark['wine_servings']+10).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc4297f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+\n",
      "|          country|beer_servings|spirit_servings|wine_servings|total_litres_of_pure_alcohol|continent|\n",
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+\n",
      "|      Afghanistan|            0|              0|            0|                         0.0|       AS|\n",
      "|          Albania|           89|            132|           54|                         4.9|       EU|\n",
      "|          Algeria|           25|              0|           14|                         0.7|       AF|\n",
      "|          Andorra|          245|            138|          312|                        12.4|       EU|\n",
      "|           Angola|          217|             57|           45|                         5.9|       AF|\n",
      "|Antigua & Barbuda|          102|            128|           45|                         4.9|       NA|\n",
      "|        Argentina|          193|             25|          221|                         8.3|       SA|\n",
      "|          Armenia|           21|            179|           11|                         3.8|       EU|\n",
      "|        Australia|          261|             72|          212|                        10.4|       OC|\n",
      "|          Austria|          279|             75|          191|                         9.7|       EU|\n",
      "|       Azerbaijan|           21|             46|            5|                         1.3|       EU|\n",
      "|          Bahamas|          122|            176|           51|                         6.3|       NA|\n",
      "|          Bahrain|           42|             63|            7|                         2.0|       AS|\n",
      "|       Bangladesh|            0|              0|            0|                         0.0|       AS|\n",
      "|         Barbados|          143|            173|           36|                         6.3|       NA|\n",
      "|          Belarus|          142|            373|           42|                        14.4|       EU|\n",
      "|          Belgium|          295|             84|          212|                        10.5|       EU|\n",
      "|           Belize|          263|            114|            8|                         6.8|       NA|\n",
      "|            Benin|           34|              4|           13|                         1.1|       AF|\n",
      "|           Bhutan|           23|              0|            0|                         0.4|       AS|\n",
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark.show()\n",
    "#As we can see that new column is not added to the original dataframe \n",
    "#Becuase there is no inplace function in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ca9690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+-------------------+\n",
      "|          country|beer_servings|spirit_servings|wine_servings|total_litres_of_pure_alcohol|continent|wine_servings_after|\n",
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+-------------------+\n",
      "|      Afghanistan|            0|              0|            0|                         0.0|       AS|                 10|\n",
      "|          Albania|           89|            132|           54|                         4.9|       EU|                 64|\n",
      "|          Algeria|           25|              0|           14|                         0.7|       AF|                 24|\n",
      "|          Andorra|          245|            138|          312|                        12.4|       EU|                322|\n",
      "|           Angola|          217|             57|           45|                         5.9|       AF|                 55|\n",
      "|Antigua & Barbuda|          102|            128|           45|                         4.9|       NA|                 55|\n",
      "|        Argentina|          193|             25|          221|                         8.3|       SA|                231|\n",
      "|          Armenia|           21|            179|           11|                         3.8|       EU|                 21|\n",
      "|        Australia|          261|             72|          212|                        10.4|       OC|                222|\n",
      "|          Austria|          279|             75|          191|                         9.7|       EU|                201|\n",
      "|       Azerbaijan|           21|             46|            5|                         1.3|       EU|                 15|\n",
      "|          Bahamas|          122|            176|           51|                         6.3|       NA|                 61|\n",
      "|          Bahrain|           42|             63|            7|                         2.0|       AS|                 17|\n",
      "|       Bangladesh|            0|              0|            0|                         0.0|       AS|                 10|\n",
      "|         Barbados|          143|            173|           36|                         6.3|       NA|                 46|\n",
      "|          Belarus|          142|            373|           42|                        14.4|       EU|                 52|\n",
      "|          Belgium|          295|             84|          212|                        10.5|       EU|                222|\n",
      "|           Belize|          263|            114|            8|                         6.8|       NA|                 18|\n",
      "|            Benin|           34|              4|           13|                         1.1|       AF|                 23|\n",
      "|           Bhutan|           23|              0|            0|                         0.4|       AS|                 10|\n",
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#There is no inplace function in Pyspark. \n",
    "#In order to reflect the output in original dataframe we have to assign it to a new variable.\n",
    "dfspark = dfspark.withColumn('wine_servings_after',dfspark['wine_servings']+10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2778281",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the column\n",
    "dfspark = dfspark.drop(\"wine_servings_after\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bd7e7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+\n",
      "|          country|beer_servings|spirit_servings|wine_servings|total_litres_of_pure_alcohol|continent|\n",
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+\n",
      "|      Afghanistan|            0|              0|            0|                         0.0|       AS|\n",
      "|          Albania|           89|            132|           54|                         4.9|       EU|\n",
      "|          Algeria|           25|              0|           14|                         0.7|       AF|\n",
      "|          Andorra|          245|            138|          312|                        12.4|       EU|\n",
      "|           Angola|          217|             57|           45|                         5.9|       AF|\n",
      "|Antigua & Barbuda|          102|            128|           45|                         4.9|       NA|\n",
      "|        Argentina|          193|             25|          221|                         8.3|       SA|\n",
      "|          Armenia|           21|            179|           11|                         3.8|       EU|\n",
      "|        Australia|          261|             72|          212|                        10.4|       OC|\n",
      "|          Austria|          279|             75|          191|                         9.7|       EU|\n",
      "|       Azerbaijan|           21|             46|            5|                         1.3|       EU|\n",
      "|          Bahamas|          122|            176|           51|                         6.3|       NA|\n",
      "|          Bahrain|           42|             63|            7|                         2.0|       AS|\n",
      "|       Bangladesh|            0|              0|            0|                         0.0|       AS|\n",
      "|         Barbados|          143|            173|           36|                         6.3|       NA|\n",
      "|          Belarus|          142|            373|           42|                        14.4|       EU|\n",
      "|          Belgium|          295|             84|          212|                        10.5|       EU|\n",
      "|           Belize|          263|            114|            8|                         6.8|       NA|\n",
      "|            Benin|           34|              4|           13|                         1.1|       AF|\n",
      "|           Bhutan|           23|              0|            0|                         0.4|       AS|\n",
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50b13a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+\n",
      "|          Country|beer_servings|spirit_servings|wine_servings|total_litres_of_pure_alcohol|continent|\n",
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+\n",
      "|      Afghanistan|            0|              0|            0|                         0.0|       AS|\n",
      "|          Albania|           89|            132|           54|                         4.9|       EU|\n",
      "|          Algeria|           25|              0|           14|                         0.7|       AF|\n",
      "|          Andorra|          245|            138|          312|                        12.4|       EU|\n",
      "|           Angola|          217|             57|           45|                         5.9|       AF|\n",
      "|Antigua & Barbuda|          102|            128|           45|                         4.9|       NA|\n",
      "|        Argentina|          193|             25|          221|                         8.3|       SA|\n",
      "|          Armenia|           21|            179|           11|                         3.8|       EU|\n",
      "|        Australia|          261|             72|          212|                        10.4|       OC|\n",
      "|          Austria|          279|             75|          191|                         9.7|       EU|\n",
      "|       Azerbaijan|           21|             46|            5|                         1.3|       EU|\n",
      "|          Bahamas|          122|            176|           51|                         6.3|       NA|\n",
      "|          Bahrain|           42|             63|            7|                         2.0|       AS|\n",
      "|       Bangladesh|            0|              0|            0|                         0.0|       AS|\n",
      "|         Barbados|          143|            173|           36|                         6.3|       NA|\n",
      "|          Belarus|          142|            373|           42|                        14.4|       EU|\n",
      "|          Belgium|          295|             84|          212|                        10.5|       EU|\n",
      "|           Belize|          263|            114|            8|                         6.8|       NA|\n",
      "|            Benin|           34|              4|           13|                         1.1|       AF|\n",
      "|           Bhutan|           23|              0|            0|                         0.4|       AS|\n",
      "+-----------------+-------------+---------------+-------------+----------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Rename the column\n",
    "dfspark = dfspark.withColumnRenamed('country','Country')\n",
    "dfspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b762672",
   "metadata": {},
   "source": [
    "# PySpark handling missing values:\n",
    "1. Dropping columns\n",
    "2. Dropping rows\n",
    "3. Various parameters involved in dropping\n",
    "4. Missing values imputations with Mean, Median and Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8800518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-BFEIRED:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Permission</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x27a1c2d50a0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark  = SparkSession.builder.appName('New').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03e54587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+---------+\n",
      "|   Name| Age|Salary|     City|\n",
      "+-------+----+------+---------+\n",
      "|  Rahul|  26| 30000|Bangalore|\n",
      "|Abhinav|  26| 34000|     Pune|\n",
      "|   Sonu|  23| 34000|   Mumbai|\n",
      "|  Akash|  24| 45000|     null|\n",
      "|  Rohit|null| 35000|   Shimla|\n",
      "|   null|  23|  null|   Manali|\n",
      "| Sanjay|  30| 56000|   Mysore|\n",
      "|   null|  21| 53000|    Hubli|\n",
      "|  Suman|  28| 45000|   Mumbai|\n",
      "|  Akash|null| 23000|Hyderabad|\n",
      "|   null|null|  null|     null|\n",
      "| Aayush|  30| 38000|    Solan|\n",
      "+-------+----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark=spark.read.csv(\"Book1.csv\",header=True, inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0840a586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|   Name| Age|Salary|\n",
      "+-------+----+------+\n",
      "|  Rahul|  26| 30000|\n",
      "|Abhinav|  26| 34000|\n",
      "|   Sonu|  23| 34000|\n",
      "|  Akash|  24| 45000|\n",
      "|  Rohit|null| 35000|\n",
      "|   null|  23|  null|\n",
      "| Sanjay|  30| 56000|\n",
      "|   null|  21| 53000|\n",
      "|  Suman|  28| 45000|\n",
      "|  Akash|null| 23000|\n",
      "|   null|null|  null|\n",
      "| Aayush|  30| 38000|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To drop a column on temporary basis:\n",
    "df_pyspark.drop('City').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29c4307b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+---------+\n",
      "|   Name|Age|Salary|     City|\n",
      "+-------+---+------+---------+\n",
      "|  Rahul| 26| 30000|Bangalore|\n",
      "|Abhinav| 26| 34000|     Pune|\n",
      "|   Sonu| 23| 34000|   Mumbai|\n",
      "| Sanjay| 30| 56000|   Mysore|\n",
      "|  Suman| 28| 45000|   Mumbai|\n",
      "| Aayush| 30| 38000|    Solan|\n",
      "+-------+---+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To drop null values\n",
    "df_pyspark.na.drop().show()\n",
    "\n",
    "#If we will not assign anything in drop() then \n",
    "#It will drop all the null values whereever it is (null) present at any row or index position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e29bc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+---------+\n",
      "|   Name|Age|Salary|     City|\n",
      "+-------+---+------+---------+\n",
      "|  Rahul| 26| 30000|Bangalore|\n",
      "|Abhinav| 26| 34000|     Pune|\n",
      "|   Sonu| 23| 34000|   Mumbai|\n",
      "| Sanjay| 30| 56000|   Mysore|\n",
      "|  Suman| 28| 45000|   Mumbai|\n",
      "| Aayush| 30| 38000|    Solan|\n",
      "+-------+---+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Drop the null values using how='any' inside drop function\n",
    "df_pyspark.na.drop(how='any').show() \n",
    "\n",
    "#It will drop all the rows wherever there is a null value in a row.\n",
    "#And by default how='any' in drop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15c8c17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+---------+\n",
      "|   Name| Age|Salary|     City|\n",
      "+-------+----+------+---------+\n",
      "|  Rahul|  26| 30000|Bangalore|\n",
      "|Abhinav|  26| 34000|     Pune|\n",
      "|   Sonu|  23| 34000|   Mumbai|\n",
      "|  Akash|  24| 45000|     null|\n",
      "|  Rohit|null| 35000|   Shimla|\n",
      "|   null|  23|  null|   Manali|\n",
      "| Sanjay|  30| 56000|   Mysore|\n",
      "|   null|  21| 53000|    Hubli|\n",
      "|  Suman|  28| 45000|   Mumbai|\n",
      "|  Akash|null| 23000|Hyderabad|\n",
      "| Aayush|  30| 38000|    Solan|\n",
      "+-------+----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Drop the null values using how='all' inside drop function\n",
    "df_pyspark.na.drop(how='all').show() \n",
    "\n",
    "#It will delete the rows whenever all the values under the columns in a row are null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ea18bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+---------+\n",
      "|   Name| Age|Salary|     City|\n",
      "+-------+----+------+---------+\n",
      "|  Rahul|  26| 30000|Bangalore|\n",
      "|Abhinav|  26| 34000|     Pune|\n",
      "|   Sonu|  23| 34000|   Mumbai|\n",
      "|  Akash|  24| 45000|     null|\n",
      "|  Rohit|null| 35000|   Shimla|\n",
      "| Sanjay|  30| 56000|   Mysore|\n",
      "|   null|  21| 53000|    Hubli|\n",
      "|  Suman|  28| 45000|   Mumbai|\n",
      "|  Akash|null| 23000|Hyderabad|\n",
      "| Aayush|  30| 38000|    Solan|\n",
      "+-------+----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#drop(Thresh=3)\n",
    "df_pyspark.na.drop(how='all', thresh=3).show()\n",
    "\n",
    "#Thresh=3 means atleast 3 non null values will be there. \n",
    "#If condition not satisfies then drop() will drop that particular row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05cdd8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+---------+\n",
      "|   Name|Age|Salary|     City|\n",
      "+-------+---+------+---------+\n",
      "|  Rahul| 26| 30000|Bangalore|\n",
      "|Abhinav| 26| 34000|     Pune|\n",
      "|   Sonu| 23| 34000|   Mumbai|\n",
      "|  Akash| 24| 45000|     null|\n",
      "|   null| 23|  null|   Manali|\n",
      "| Sanjay| 30| 56000|   Mysore|\n",
      "|   null| 21| 53000|    Hubli|\n",
      "|  Suman| 28| 45000|   Mumbai|\n",
      "| Aayush| 30| 38000|    Solan|\n",
      "+-------+---+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#drop(subset='Age')\n",
    "df_pyspark.na.drop(subset='Age').show()\n",
    "\n",
    "#It will drop all the rows wherever the age values are null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8cc5cc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+---------+\n",
      "|   Name|Age|Salary|     City|\n",
      "+-------+---+------+---------+\n",
      "|  Rahul| 26| 30000|Bangalore|\n",
      "|Abhinav| 26| 34000|     Pune|\n",
      "|   Sonu| 23| 34000|   Mumbai|\n",
      "|  Akash| 24| 45000|     null|\n",
      "|  Rohit|  0| 35000|   Shimla|\n",
      "|   null| 23|     0|   Manali|\n",
      "| Sanjay| 30| 56000|   Mysore|\n",
      "|   null| 21| 53000|    Hubli|\n",
      "|  Suman| 28| 45000|   Mumbai|\n",
      "|  Akash|  0| 23000|Hyderabad|\n",
      "|   null|  0|     0|     null|\n",
      "| Aayush| 30| 38000|    Solan|\n",
      "+-------+---+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filling the missing values with 0 in Age and Salary column:\n",
    "df_pyspark.na.fill(0,['Salary','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89a49f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+---------+\n",
      "|   Name| Age|Salary|     City|\n",
      "+-------+----+------+---------+\n",
      "|  Rahul|  26| 30000|Bangalore|\n",
      "|Abhinav|  26| 34000|     Pune|\n",
      "|   Sonu|  23| 34000|   Mumbai|\n",
      "|  Akash|  24| 45000|  Missing|\n",
      "|  Rohit|null| 35000|   Shimla|\n",
      "|Missing|  23|  null|   Manali|\n",
      "| Sanjay|  30| 56000|   Mysore|\n",
      "|Missing|  21| 53000|    Hubli|\n",
      "|  Suman|  28| 45000|   Mumbai|\n",
      "|  Akash|null| 23000|Hyderabad|\n",
      "|Missing|null|  null|  Missing|\n",
      "| Aayush|  30| 38000|    Solan|\n",
      "+-------+----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filling the missing values with 'Missing' in Name column:\n",
    "df_pyspark.na.fill('Missing',['Name','City']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc07a04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+---------+\n",
      "|   Name|Age|Salary|     City|\n",
      "+-------+---+------+---------+\n",
      "|  Rahul| 26| 30000|Bangalore|\n",
      "|Abhinav| 26| 34000|     Pune|\n",
      "|   Sonu| 23| 34000|   Mumbai|\n",
      "|  Akash| 24| 45000|     null|\n",
      "|  Rohit| 25| 35000|   Shimla|\n",
      "|   null| 23| 39300|   Manali|\n",
      "| Sanjay| 30| 56000|   Mysore|\n",
      "|   null| 21| 53000|    Hubli|\n",
      "|  Suman| 28| 45000|   Mumbai|\n",
      "|  Akash| 25| 23000|Hyderabad|\n",
      "|   null| 25| 39300|     null|\n",
      "| Aayush| 30| 38000|    Solan|\n",
      "+-------+---+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filling the missing values with mean using imputer:\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(inputCols=['Age','Salary'],\n",
    "                 outputCols=['Age','Salary']).setStrategy(\"mean\")\n",
    "\n",
    "\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4ee3473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+---------+\n",
      "|   Name|Age|Salary|     City|\n",
      "+-------+---+------+---------+\n",
      "|  Rahul| 26| 30000|Bangalore|\n",
      "|Abhinav| 26| 34000|     Pune|\n",
      "|   Sonu| 23| 34000|   Mumbai|\n",
      "|  Akash| 24| 45000|     null|\n",
      "|  Rohit| 26| 35000|   Shimla|\n",
      "|   null| 23| 35000|   Manali|\n",
      "| Sanjay| 30| 56000|   Mysore|\n",
      "|   null| 21| 53000|    Hubli|\n",
      "|  Suman| 28| 45000|   Mumbai|\n",
      "|  Akash| 26| 23000|Hyderabad|\n",
      "|   null| 26| 35000|     null|\n",
      "| Aayush| 30| 38000|    Solan|\n",
      "+-------+---+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filling the missing values with median using imputer:\n",
    "\n",
    "\n",
    "imputer = Imputer(inputCols=['Age','Salary'],\n",
    "                 outputCols=['Age','Salary']).setStrategy(\"median\")\n",
    "\n",
    "\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62febb98",
   "metadata": {},
   "source": [
    "# PySpark dataframe\n",
    "1. Filter options\n",
    "2. &,|,==\n",
    "3. ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6a55965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+---------+\n",
      "|   Name| Age|Salary|     City|\n",
      "+-------+----+------+---------+\n",
      "|  Rahul|  26| 30000|Bangalore|\n",
      "|Abhinav|  26| 34000|     Pune|\n",
      "|   Sonu|  23| 34000|   Mumbai|\n",
      "|  Akash|  24| 45000|     null|\n",
      "|  Rohit|null| 35000|   Shimla|\n",
      "|   null|  23|  null|   Manali|\n",
      "| Sanjay|  30| 56000|   Mysore|\n",
      "|   null|  21| 53000|    Hubli|\n",
      "|  Suman|  28| 45000|   Mumbai|\n",
      "|  Akash|null| 23000|Hyderabad|\n",
      "|   null|null|  null|     null|\n",
      "| Aayush|  30| 38000|    Solan|\n",
      "+-------+----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9ad0659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+---------+\n",
      "| Name|Age|Salary|     City|\n",
      "+-----+---+------+---------+\n",
      "|Rahul| 26| 30000|Bangalore|\n",
      "+-----+---+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select records where name is Rahul\n",
    "df_pyspark.where(df_pyspark['Name']=='Rahul').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b549b7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+------+\n",
      "|  Name| Age|Salary|  City|\n",
      "+------+----+------+------+\n",
      "| Akash|  24| 45000|  null|\n",
      "| Rohit|null| 35000|Shimla|\n",
      "|Sanjay|  30| 56000|Mysore|\n",
      "|  null|  21| 53000| Hubli|\n",
      "| Suman|  28| 45000|Mumbai|\n",
      "|Aayush|  30| 38000| Solan|\n",
      "+------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select the records where salary greater than 35000\n",
    "df_pyspark.where(df_pyspark['Salary']>=35000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "41e8fd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+------+\n",
      "|  Name| Age|Salary|  City|\n",
      "+------+----+------+------+\n",
      "| Akash|  24| 45000|  null|\n",
      "| Rohit|null| 35000|Shimla|\n",
      "|Sanjay|  30| 56000|Mysore|\n",
      "|  null|  21| 53000| Hubli|\n",
      "| Suman|  28| 45000|Mumbai|\n",
      "|Aayush|  30| 38000| Solan|\n",
      "+------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select the records where salary greater than 35000\n",
    "df_pyspark.filter(df_pyspark['Salary']>=35000).show()\n",
    "\n",
    "\n",
    "#We can use where and filter to filter out the records from the columns based on the conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f4e2c54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|  Name| Age|\n",
      "+------+----+\n",
      "| Akash|  24|\n",
      "| Rohit|null|\n",
      "|Sanjay|  30|\n",
      "|  null|  21|\n",
      "| Suman|  28|\n",
      "|Aayush|  30|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select the name and age of the employees where salary:\n",
    "\n",
    "df_pyspark.filter(df_pyspark['Salary']>=35000).select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8c6a8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+------+\n",
      "|  Name|Age|Salary|  City|\n",
      "+------+---+------+------+\n",
      "|Sanjay| 30| 56000|Mysore|\n",
      "| Suman| 28| 45000|Mumbai|\n",
      "|Aayush| 30| 38000| Solan|\n",
      "+------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select the records where Salary more than 35000 and Age more than 25: \n",
    "\n",
    "df_pyspark.filter((df_pyspark['Salary']>=35000) & (df_pyspark['Age']>25)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "62a59d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+---------+\n",
      "|   Name| Age|Salary|     City|\n",
      "+-------+----+------+---------+\n",
      "|  Rahul|  26| 30000|Bangalore|\n",
      "|Abhinav|  26| 34000|     Pune|\n",
      "|   Sonu|  23| 34000|   Mumbai|\n",
      "|  Akash|null| 23000|Hyderabad|\n",
      "+-------+----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ~ filter operation:\n",
    "df_pyspark.filter(~(df_pyspark['Salary']>=35000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d15d95",
   "metadata": {},
   "source": [
    "# PySpark\n",
    "1. GroupBy\n",
    "2. Aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e9d4704d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-BFEIRED:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Permission</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x27a1c2d50a0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To create a spark session:\n",
    "spark = SparkSession.builder.appName('Data').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2d33d140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-----+\n",
      "|     City|State| Sale|\n",
      "+---------+-----+-----+\n",
      "|   Shimla|   HP|82690|\n",
      "|   Mumbai|   MH|64217|\n",
      "|Bangalore|   KA|10671|\n",
      "|   Mumbai|   MH|89811|\n",
      "|   Mumbai|   MH| 8583|\n",
      "|   Shimla|   HP|88233|\n",
      "|    Solan|   HP| 5546|\n",
      "|   Shimla|   HP|33440|\n",
      "|    Solan|   HP|51400|\n",
      "|Bangalore|   KA|60855|\n",
      "|Bangalore|   KA|94763|\n",
      "|Bangalore|   KA|31957|\n",
      "|   Shimla|   HP|39926|\n",
      "|Hyderabad|   TS|77598|\n",
      "|  Chennai|   TN|84379|\n",
      "|  Chennai|   TN|93683|\n",
      "|    Solan|   HP|18894|\n",
      "|    Solan|   HP|78674|\n",
      "|   Mumbai|   MH|14280|\n",
      "|   Shimla|   HP|43004|\n",
      "+---------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To read a csv file:\n",
    "sale_pyspark = spark.read.csv('Sale.csv', header=True, inferSchema=True)\n",
    "sale_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "22c0505a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Sale: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To check schema\n",
    "sale_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e993311c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|     City|max(Sale)|\n",
      "+---------+---------+\n",
      "|Bangalore|    97480|\n",
      "|  Chennai|    93683|\n",
      "|   Shimla|    88233|\n",
      "|   Mumbai|    89811|\n",
      "|     Pune|    63951|\n",
      "|    Delhi|    76151|\n",
      "|    Solan|    78674|\n",
      "|Hyderabad|    77598|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Group by city to check maximum sale city wise:\n",
    "sale_pyspark.groupBy(['City']).max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a093699f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|State|max(Sale)|\n",
      "+-----+---------+\n",
      "|   HP|    88233|\n",
      "|   DL|    76151|\n",
      "|   TS|    77598|\n",
      "|   TN|    93683|\n",
      "|   MH|    89811|\n",
      "|   KA|    97480|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Group by city to check maximum sale state wise:\n",
    "sale_pyspark.groupBy(['State']).max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b28aa710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|     City|sum(Sale)|\n",
      "+---------+---------+\n",
      "|Bangalore|   380215|\n",
      "|  Chennai|   326005|\n",
      "|   Shimla|   287293|\n",
      "|   Mumbai|   176891|\n",
      "|     Pune|   139120|\n",
      "|    Delhi|   139530|\n",
      "|    Solan|   154514|\n",
      "|Hyderabad|   156403|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Group by city to check summation of sale city wise:\n",
    "sale_pyspark.groupBy(['City']).sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9ce64223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|State|count|\n",
      "+-----+-----+\n",
      "|   HP|    9|\n",
      "|   DL|    3|\n",
      "|   TS|    3|\n",
      "|   TN|    4|\n",
      "|   MH|    8|\n",
      "|   KA|    6|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Group by state to check count of city in each state:\n",
    "sale_pyspark.groupBy(['State']).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "96af17fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|sum(Sale)|\n",
      "+---------+\n",
      "|  1759971|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Aggregate function to check total sale\n",
    "sale_pyspark.agg({'Sale':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a41c77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46041cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63384a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e3e51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e227be20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
